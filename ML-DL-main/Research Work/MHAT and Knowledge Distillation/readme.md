# MHAT and Knowledge Distillation

This is a research work on the MHAT (Model Heterogeneous Aggregation Techniques) and Knowledge Distillation where I worked with my other group members as a team work to find novelty.

## Our Novelty
> We came up with an idea of integration of both of these techniques that will finally give to rise to a new technique. This enhanced our accuracy and model extracting properties of teacher and student.

For a better understanding of it, you would definitely have prior knowledge about these techniques.

## Links 

[Knowledge Distillation](https://neptune.ai/blog/knowledge-distillation)
[MHAT (Model Heterogeneous Aggregation Technique)](https://www.sciencedirect.com/science/article/abs/pii/S0020025521000840)